\chapter{Important Concepts}
\label{chap:ic}

This appendix is a collection of important mathematical concepts that are frequently used in software engineering. 
The content of this appendix is based on the concepts in this book.

\begin{proposition}{Order of Operations}
To evaluate mathematical expressions, operations are performed in the following order:
\begin{enumerate}
    \item \textbf{Brackets (Parentheses):} First, perform all operations inside brackets or parentheses.
    \item \textbf{Exponents and Radicals:} Next, evaluate exponents (powers) and radicals (roots).
    \item \textbf{Multiplication and Division:} Then, perform multiplication and division from left to right.
    \item \textbf{Addition and Subtraction:} Finally, execute addition and subtraction from left to right.
\end{enumerate}
\end{proposition}

\begin{proposition}[Rules for Calculations with Fractions]
For $a,b,c,m \in \mathbb{R}$, with $a,b,c,m \neq 0$ where required, the following identities hold:
\begingroup
\setlength{\jot}{8pt} % increase space between align rows (default ~3pt)
\begin{align*}
(1) & \quad \frac{a}{b} \times m = \frac{am}{b} \\
(2) & \quad \frac{a}{b} \div m = \frac{a}{bm} \\
(3) & \quad m \div \frac{a}{b} = \frac{mb}{a} \\
(4) & \quad \frac{a}{b} \times \frac{c}{a} = \frac{c}{b} \\
(5) & \quad \frac{a}{b} \div \frac{c}{a} = \frac{a^2}{bc} \\
(6) & \quad \frac{a}{b} = \frac{ac}{bc} \\
(7) & \quad \frac{a}{b} + \frac{c}{a} = \frac{a^2 + bc}{ab}
\end{align*}
\endgroup
\end{proposition}

\begin{proposition}{Properties of Integer Exponents}
Let $n,m\in\mathbb{Z}$. Then the following hold (with $x,y\in\mathbb{R}$ and nonzero where stated):
\[
\begin{aligned}
&\text{(1)}\quad x^{n}\cdot x^{m}=x^{\,n+m},\\[2pt]
&\text{(2)}\quad \dfrac{x^{n}}{x^{m}}=x^{\,n-m}\quad \text{with } x\neq 0,\\[2pt]
&\text{(3)}\quad x^{n}\cdot y^{n}=(xy)^{n},\\[2pt]
&\text{(4)}\quad \dfrac{x^{n}}{y^{n}}=\left(\dfrac{x}{y}\right)^{n}\quad \text{with } y\neq 0,\\[2pt]
&\text{(5)}\quad \bigl(x^{n}\bigr)^{m}=x^{\,nm},\\[2pt]
&\text{(6)}\quad x^{1}=x.
\end{aligned}
\]
\end{proposition}

\begin{proposition}{More Properties of Integer Exponents}
Let $n,m\in\mathbb{Z}$. Then the following hold (with $x,y\in\mathbb{R}$ and nonzero where stated):
\[
\begin{aligned}
&\text{(7)}\quad x^{0}=1   &\qquad&   x \neq 0  \\
&\text{(8)}\quad \frac{1}{x^{m}}=x^{-m} &\qquad& x \neq 0 \\
\end{aligned}
\]
\end{proposition}

\begin{custombox}{Rules for rearranging formulae}
The following operations can be performed on both sides of the formula:
\begin{itemize}
    \item Add the same quantity to both sides
    \item Subtract the same quantity from both sides
    \item Multiply both sides by the same quantity - remember to multiply all terms
    \item Divide both sides by the same quantity - remember to divide all terms
    \item Apply a function to both sides, such as squaring or finding the reciprocal
\end{itemize}
\end{custombox}

\begin{definition} {Injective and Surjective Functions}
A function \( f: A \rightarrow B \) is called \textbf{one-to-one} (or \textbf{injective}) if different elements in \( A \) map to different elements in \( B \). 
A function \( f: A \rightarrow B \) is called \textbf{onto} (or \textbf{surjective}) if every element in \( B \) is the image of at least one element in \( A \).
\end{definition}

\begin{definition}{Inverse Functions}
Let $f$ be a one-to-one correspondence from the set $A$ to the set $B$. The inverse function of $f$ is the function that assigns to an element $b$ belonging to $B$ the unique element $a$ in $A$ such that $f(a)=b$. The inverse function of $f$ is denoted by $f^{-1}$. Hence, $f^{-1}(b)=a$ when $f(a)=b$.    
\end{definition}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figure/book3.png} % Adjust width here to scale the image
    \caption{Power functions}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figure/book4.png} % Adjust width here to scale the image
    \caption{Exponential functions}
\end{figure}

\begin{definition}{Base-10 Logarithms}

\[
\log x=y \iff 10^y=x
\]

\textit{Verbally}: $\log x$ is the exponent in the power of 10 that gives $x$   
\end{definition}

\begin{custombox}{Properties of base-10 logarithms}
\begin{itemize}
    \item Log of a Product:
    
    \[
    \log x y=\log x+\log y
    \]
    
    \textit{Verbally}: The $\log$ of a product equals the sum of the logs of the factors.
    \vspace{0.2cm}
    \item Log of a Quotient:
    
    \[
    \log \frac{x}{y}=\log x-\log y
    \]
    \vspace{0.1cm}
    \textit{Verbally}: The $\log$ of a quotient equals the log of the numerator minus the $\log$ of the denominator.
    \vspace{0.2cm}
    \item Log of a Power:
    \[
    \log x^y=y \log x
    \]
    \textit{Verbally}: The $\log$ of a power equals the exponent times the log of the base.
\end{itemize}

   
\end{custombox}

\begin{definition}{Common Logarithm and Natural Logarithm}
\hspace{1cm} \textit{Common}: The symbol $\log x$ means $\log _{10} x$.

\hspace{1cm}  \textit{Natural}: \hspace{0.2cm}The symbol $\ln x$ means $\log _e x$, where $e$ is a constant equal to $2.71828182845 \ldots$
\end{definition}

\begin{custombox}{The Change-of-Base Property of Logarithms}

\begin{equation*}
\log _a x=\frac{\log _b x}{\log _b a} \quad \text { or } \quad \log _a x=\frac{1}{\log _b a}\left(\log _b x\right)
\end{equation*}
    
\end{custombox}

\begin{custombox}{Properties of Logarithms}
\setlength{\leftskip}{1cm}  % Reset the text indent
\setlength{\rightskip}{1cm} % Reset the right indent
The Logarithm of a Power:
$$
\log _b x^y=y \log _b x
$$
The Logarithm of a Product:
$$
\log _b(x y)=\log _b x+\log _b y
$$
The Logarithm of a Quotient:
$$
\log _b \frac{x}{y}=\log _b x-\log _b y
$$
\setlength{\leftskip}{0cm}  % Reset the text indent
\setlength{\rightskip}{0cm} % Reset the right indent

\end{custombox}

% Additions for Chapter 02: Number Systems

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Numeral system} & \textbf{Symbols} & \textbf{Base} & \textbf{Additional information} \\ \hline
\textbf{Decimal} & 0-9 & 10 & - \\ \hline
\textbf{Binary} & 0, 1 & 2 & - \\ \hline
\textbf{Hexadecimal} & 0-9, A-F & 16 & $\mathrm{A} \equiv 10, \mathrm{B} \equiv 11, \mathrm{C} \equiv 12,$ $\mathrm{D} \equiv 13, \mathrm{E} \equiv 14, \mathrm{F} \equiv 15$ \\ \hline
\textbf{Octal} & 0-7 & 8 & - \\ \hline
\end{tabular}
\caption{Summary of Common Numeral Systems}
\end{table}

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c c c c c c c c|}
\hline 
\multirow{2}{*}{Decimal number} & & \multirow{2}{*}{In powers of 2} & \multicolumn{4}{c}{Power of 2} & \multirow{2}{*}{Binary number} \\
& & & 3 & 2 & 1 & 0 & \\ \hline
8 & $=$ & $2^3$ & 1 & 0 & 0 & 0 & 1000 \\
7 & $=$ & $2^2 + 2^1 + 2^0$ & 0 & 1 & 1 & 1 & 111 \\
6 & $=$ & $2^2 + 2^1$ & 0 & 1 & 1 & 0 & 110 \\
5 & $=$ & $2^2 + 2^0$ & 0 & 1 & 0 & 1 & 101 \\
4 & $=$ & $2^2$ & 0 & 1 & 0 & 0 & 100 \\
3 & $=$ & $2^1 + 2^0$ & 0 & 0 & 1 & 1 & 11 \\
2 & $=$ & $2^1$ & 0 & 0 & 1 & 0 & 10 \\
1 & $=$ & $2^0$ & 0 & 0 & 0 & 1 & 1 \\ \hline
\end{tabular}
\caption{Decimal Numbers in Binary Representation}
\end{table}

\begin{proposition}{Binary Addition Rules}
\[
0 + 0 = 0, \quad 0 + 1 = 1, \quad 1 + 0 = 1, \quad 1 + 1 = 10
\]
\end{proposition}

\begin{proposition}{Binary Multiplication Rules}
\[
\begin{aligned}
& 0 \times 0=0 \\
& 0 \times 1=0 \\
& 1 \times 0=0 \\
& 1 \times 1=1 \\
& 1 \times 10_2 = 10_2 \quad \left( \text{multiplying by base } 10_2 \text{ adds a 0 to the end} \right)
\end{aligned}
\]
\end{proposition}

\begin{proposition}{XOR Operation}
XOR produces a 1 if the two bits being compared are different and a 0 if they are the same:
\[
0 \oplus 0 = 0, \quad 0 \oplus 1 = 1, \quad 1 \oplus 0 = 1, \quad 1 \oplus 1 = 0
\]
\end{proposition}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[>=Stealth, node distance=4cm, thick, scale=1]

    % Draw larger circles for sets A and B
    \draw[thick] (0,0) circle [radius=1.5cm] node at (0, 2) {\Large $A$};
    \draw[thick] (6,0) circle [radius=1.5cm] node at (6, 2) {\Large $B$};

    % Place larger nodes for elements a and b=f(a)
    \node[fill, circle, inner sep=2pt, label=below:{$a$}] (a) at (0,0.3) {};
    \node[fill, circle, inner sep=2pt, label=below:{$b=f(a)$}] (b) at (6,0.3) {};

    % Draw the larger straight arrow representing f
    \draw[->, ultra thick, black] (a) -- (b) node[midway, above] {\Large $f$};

    % Draw the curved arrow representing f at the bottom, touching the circles
    \draw[->, ultra thick, black, bend right=30] ([shift={(270:1.5cm)}]0,0) 
    to node[midway, below] {\Large $f$} 
    ([shift={(270:1.5cm)}]6,0);

\end{tikzpicture}
\caption{A function \(f\) mapping an element \(a\) from set \(A\) to an element \(b=f(a)\) in set \(B\).}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[>=Stealth, thick, scale = 1.5]

% Draw circles representing sets A and B
\draw (0,0) circle (1.5cm);
\draw (6,0) circle (1.5cm);

% Labels for sets A and B
\node at (0,-1) {A};
\node at (6,-1) {B};


% Points in sets with adjusted labels
\node[fill=black, circle, inner sep=1.5pt, label={[xshift=-0.6cm]below:{$a = f^{-1}(b)$}}] (A1) at (0,0.7) {};
\node[fill=black, circle, inner sep=1.5pt, label={[xshift=0.5cm]below:{$b = f(a)$}}] (B1) at (6,0.7) {};


% Arrows for f and f^{-1}
\draw[->] (A1) to[bend left=30] node[midway, above] {$f(a)$} (B1);
\draw[->] (B1) to[bend left=30] node[midway, below] {$f^{-1}(b)$} (A1);

% Arrows for f and f^{-1} below the main circles
\draw[->] (0.6,-1.5) to[bend left=20] node[midway, above] {$f$} (5.4,-1.5);
\draw[->] (5.4,-1.8) to[bend left=20] node[midway, below] {$f^{-1}$} (0.6,-1.8);

\end{tikzpicture}
\caption{The function $f^{-1}$ is the inverse of function $f$.}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[>=Stealth, thick]

% Draw circles representing sets A, B, and C
\draw (0,0) circle (1.5cm);  % Set A
\draw (5,0) circle (1.5cm);  % Set B
\draw (10,0) circle (1.5cm); % Set C

% Labels for sets A, B, and C
\node at (0,-1) {A};
\node at (5,-1) {B};
\node at (10,-1) {C};

% Points in sets with adjusted labels
\node[fill=black, circle, inner sep=1.5pt, label={[xshift=-0.1cm]below:{$a$}}] (A) at (0,0.7) {};

\node[fill=black, circle, inner sep=1.5pt, label=below:{$g(a)$}] (B) at (5,0.7) {};
\node[fill=black, circle, inner sep=1.5pt, label={[xshift=0.4cm]below:{$f(g(a))$}}] (C) at (10,0.7) {};

% Arrows for g and f
\draw[->] (A) to[bend left=15] node[midway, above] {$g(a)$} (B);
\draw[->] (B) to[bend left=15] node[midway, above] {$f(g(a))$} (C);
\draw[->] ([shift={(360:1.5cm)}]0,0) to node[midway, below] {$g$} (3.5,0);
\draw[->] ([shift={(360:0cm)}]6.5,0) to node[midway, below] {$f$} (8.5,0);

% Arrows for f ∘ g and (f ∘ g)(a)
\draw[->] (A) to[bend left=40] node[midway, above] {$(f \circ g)(a)$} (C);
% Draw the curved arrow representing f at the bottom, touching the circles
\draw[->, bend right=20] ([shift={(270:1.5cm)}]0,0) to node[midway, below] {$f(g(a))$} ([shift={(270:0.7cm)}]10,0);

\end{tikzpicture}
\caption{The composition of functions \(f\) and \(g\), denoted \(f \circ g\), is the function that results from applying \(g\) and then \(f\).}
\end{figure}

% ---------- 04 Combinatorics & Probability ----------

\begin{custombox}{Sample Space \& Events (Set–Logic Dictionary)}
\begin{tabular}{ll}
Event complement & $A^c$ \\[2pt]
Union (OR) & $A\cup B$ \\[2pt]
Intersection (AND) & $A\cap B$ \\[2pt]
Difference & $A\setminus B$ \\[2pt]
Symmetric difference & $A\triangle B=(A\setminus B)\cup(B\setminus A)$ \\[4pt]
De Morgan & $(A\cup B)^c=A^c\cap B^c,\quad (A\cap B)^c=A^c\cup B^c$
\end{tabular}
\end{custombox}

\begin{proposition}{Factorial \& Binomial Coefficient Identities}
\[
n!=n\,(n-1)\cdots 2\cdot 1,\qquad 0!=1
\]
\[
\binom{n}{r}=\frac{n!}{r!(n-r)!},\quad \binom{n}{r}=\binom{n}{n-r},\quad
\binom{n}{r}=\binom{n-1}{r}+\binom{n-1}{r-1}
\]
\end{proposition}

\begin{custombox}{Counting: Ordered/Unordered, With/Without Replacement}
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|c|}
\hline
Selection of $r$ from $n$ & \textbf{Ordered} & \textbf{Unordered} \\
\hline
Without replacement & $P(n,r)=\dfrac{n!}{(n-r)!}$ & $\displaystyle \binom{n}{r}$ \\
\hline
With replacement & $n^r$ & $\displaystyle \binom{n+r-1}{r}$ \\
\hline
\end{tabular}
\end{custombox}

\begin{proposition}{Linear Permutations \& Subset Permutations}
All arrangements of $n$ distinct objects: $n!$. \quad
Arrangements of $r$ out of $n$ (order matters): $P(n,r)=\dfrac{n!}{(n-r)!}$.
\end{proposition}

\begin{proposition}{Handshakes / Complete Graph Edges}
Number of unordered pairs among $n$ people (each pair shakes once):
\[
\binom{n}{2}=\frac{n(n-1)}{2}
\]
\end{proposition}

\begin{proposition}{Binomial Theorem (Coefficient Form)}
\[
(a+b)^n=\sum_{k=0}^{n}\binom{n}{k}\,a^{k}b^{\,n-k}
\]
\end{proposition}

\begin{custombox}{Axioms \& Core Probability Rules}
\textbf{Axioms:}\ $0\le P(A)\le 1$;\ $P(S)=1$;\ if $A_i$ disjoint then $P\!\left(\bigcup_i A_i\right)=\sum_i P(A_i)$.

\medskip
\textbf{Consequences:}
\[
P(A^c)=1-P(A),\quad P(\emptyset)=0,\quad
P(A\cup B)=P(A)+P(B)-P(A\cap B),
\]
\[
P(A\setminus B)=P(A)-P(A\cap B),\quad A\subseteq B\Rightarrow P(A)\le P(B).
\]
\end{custombox}

\begin{proposition}{Two- and Three-Event Inclusion–Exclusion}
\[
P(A\cup B)=P(A)+P(B)-P(A\cap B)
\]
\[
\begin{aligned}
P(A\cup B\cup C)=\ &P(A)+P(B)+P(C) \\
&-\,P(A\cap B)-P(A\cap C)-P(B\cap C) \\
&+\,P(A\cap B\cap C)
\end{aligned}
\]
\end{proposition}

\begin{custombox}{Union Bounds (Quick Estimates)}
\[
P(A\cup B)\le P(A)+P(B)
\qquad\text{and, more generally,}\qquad
P\!\left(\bigcup_{i=1}^m A_i\right)\le \sum_{i=1}^m P(A_i)
\]
\end{custombox}

\begin{proposition}{Finite Uniform Sample Space}
If $S$ has $|S|$ equally likely outcomes and $A\subseteq S$, then
\[
P(A)=\frac{|A|}{|S|}.
\]
\end{proposition}

\begin{custombox}{Replacement \& Independence (Coins/Cards intuition)}
With replacement across trials $\Rightarrow$ identical single-trial probabilities (often independent).
Without replacement $\Rightarrow$ changing denominators; probabilities update each draw.
\end{custombox}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0,thick,>=Stealth]
\draw (0,0) circle (2cm);
\draw (1.4,0) circle (2cm);
\node at (-1.2,1.2) {$A$};
\node at (2.6,1.2) {$B$};
\node at (-2.4,0) {$A^c$};
\node at (4.2,0) {$B^c$};
% annotate regions
\node at (-0.9,0) {$A\setminus B$};
\node at (0.7,0) {$A\cap B$};
\node at (2.2,0) {$B\setminus A$};
\end{tikzpicture}
\caption*{Venn aide-mémoire for $A\cup B$, $A\cap B$, complements, and differences.}
\end{figure}

% ---------- 05 Conditional Probability ----------

\begin{definition}[Conditional Probability]
For any two events \( A \) and \( B \) with \( P(B) > 0 \), the conditional probability of \( A \) given \( B \) is defined as:
\[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \qquad P(B) > 0
\]
\end{definition}

\begin{definition}[Independence]
    Two events are considered independent if the occurrence of one event does not affect the probability of the other event. In other words, the probability of one event does not depend on the occurrence of the other event. Two events \( A \) and \( B \) are independent if:
    \begin{align*}
        P(A \mid B) &= P(A) \\
        P(B \mid A) &= P(B) \\
        P(A \cap B) &= P(A) P(B)
    \end{align*}
\end{definition}

\begin{definition}[Independence of Multiple Events]
    A set of events \( A_1, A_2, \ldots, A_k \) are considered independent if  the joint probability is equal to the product of the individual probabilities:
    \[
    P(A_1 \cap A_2 \cap \cdots \cap A_k) = P(A_1) \cdot P(A_2) \cdots P(A_k)
    \]
\end{definition}

\begin{axiom}[Axioms of Conditional Probability]
    \begin{itemize}
        \item \textbf{Axiom 1:} For any event \(A\), \( 0 \leq P(A\mid B) \leq 1 \).
        \item \textbf{Axiom 2:} Conditional probability of $B$ given $B$ is 1, i.e., $P(B \mid B)=1$.
        \item \textbf{Axiom 3:} If \(A_1, A_2, A_3, \cdots\) are disjoint events, then
         
        \hspace*{2.8em}$P\left(A_1 \cup A_2 \cup A_3 \cdots \mid B\right)=P\left(A_1 \mid B\right)+P\left(A_2 \mid B\right)+P\left(A_3 \mid B\right)+\cdots$
    \end{itemize}
\end{axiom}

\begin{theorem}{Rules of Conditional Probability}
    \begin{itemize}
        \item \textbf{Complement Rule:} The probability of the complement of event $A$ given $C$ is
    
        \[
        P\left(A^c \mid C\right)=1-P(A \mid C)
        \]

        \item \textbf{Empty Set Rule:} The probability of the empty set given some event $C$ is 0, i.e.,
        
        \[P(\emptyset \mid C) = 0\]
        
        \item \textbf{Addition Rule:} For any two events $A$ and $B$, the probability of the union of events $A$ and $B$ given another event $C$ is
        \[
        P(A \cup B \mid C)=P(A \mid C)+P(B \mid C)-P(A \cap B \mid C)
        \]

        \item \textbf{Difference Rule:} The probability of the difference between events $A$ and $B$ given another event $C$ is
        \[
            P(A-B \mid C)=P(A \mid C)-P(A \cap B \mid C)
        \]
        \item \textbf{Subset Rule:} If $A \subset B$, then
        \[
        P(A \mid C) \leq P(B \mid C)
        \]
    \end{itemize}
\end{theorem}

\begin{theorem}[Multiplication Rule]
    For any two events \( A \) and \( B \), the probability of both events occurring is given by:
    \[
    P(A \cap B) = P(A) P(B \mid A) = P(B) P(A \mid B)
    \]
\end{theorem}

\begin{theorem}[Law of Total Probability]
For any events \( A \) and \( B \) such that \( B \) and \( B^c \) form a partition of the sample space \( S \), the total probability of event \( A \) is given by:
    \begin{align*}
    P(A) &= P(A \cap B) + P(A \cap B^c) \\
    &= P(B) P(A \mid B) + P(B^c) P(A \mid B^c)
    \end{align*}

    For any event \( A \) and any partition of the sample space \( B_1, B_2, \ldots, B_n \) such that \( B_i \cap B_j = \emptyset \) for all \( i \neq j \) and \( \bigcup_{i=1}^{n} B_i = S \), the total probability of event \( A \) is given by:
    \begin{align*}
    P(A) &= \sum_{i=1}^{n} P(A \mid B_i) P(B_i)
    \end{align*}
\end{theorem}

\begin{theorem}[Independence and DeMorgan's Law]
        If \( A_1, A_2, \cdots, A_n \) are independent then
        \begin{align*}
        &P\left(A_1 \cup A_2 \cup \cdots \cup A_n\right)=1-\left(1-P\left(A_1\right)\right)\left(1-P\left(A_2\right)\right) \cdots\left(1-P\left(A_n\right)\right)
        \end{align*}
\end{theorem}

\begin{theorem}[Bayes' Theorem]
    For any two events $A$ and $B$, where $P(A) \neq 0$, we have
    \begin{align*}
    P(B \mid A) = \frac{P(A \mid B) \cdot P(B)}{P(A)}
     = \frac{P(A \mid B) \cdot P(B)}{P(A \mid B) \cdot P(B) + P(A \mid B^c) \cdot P(B^c)}
    \end{align*}

    If $B_1, B_2, B_3, \cdots$ form a partition of the sample space $S$, and $A$ is any event with $P(A) \neq 0$, we have

    \begin{align*}
    P\left(B_j \mid A\right) = \frac{P\left(A \mid B_j\right) \cdot P\left(B_j\right)}{\sum_i P\left(A \mid B_i\right) \cdot P\left(B_i\right)}
    \end{align*}
\end{theorem}

% ---------- 06 Descriptive Statistics ----------

\begin{definition}[Population]
A \textbf{population} is the complete set of all possible observations or measurements of interest in a particular study. In software engineering contexts, this might include all possible execution times of an algorithm, all user sessions on a website, or all lines of code in a project.
\end{definition}

\begin{definition}[Sample]
A \textbf{sample} is a subset of the population that we actually observe or measure. Due to practical constraints, we often work with samples rather than entire populations.
\end{definition}

\begin{definition}[Qualitative Data]
\textbf{Qualitative data} (also called categorical data) consists of non-numerical information that can be categorized. Examples include programming languages used in a project, user satisfaction ratings (satisfied/neutral/dissatisfied), or bug severity levels (critical/high/medium/low).
\end{definition}

\begin{definition}[Quantitative Data]
\textbf{Quantitative data} consists of numerical measurements that can be ordered and subjected to mathematical operations. This can be further divided into:
\begin{itemize}
    \item \textbf{Discrete}: Countable values (number of bugs, lines of code, user sessions)
    \item \textbf{Continuous}: Measurable values that can take any value within a range (response times, memory usage, CPU utilization)
\end{itemize}
\end{definition}

\begin{definition}[Frequency Distribution]
A \textbf{frequency distribution} is a table that shows the frequency (count) of each value or class of values in a dataset. It can be presented as:
\begin{itemize}
    \item \textbf{Absolute frequency}: The actual count of occurrences
    \item \textbf{Relative frequency}: The proportion of total observations
    \item \textbf{Cumulative frequency}: The running total of frequencies
\end{itemize}
\end{definition}

\begin{definition}[Arithmetic Mean]

For a dataset with $n$ values $x_1, x_2, \ldots, x_n$, the \textbf{arithmetic mean} is defined as:

$$
\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i = \frac{x_1 + x_2 + \cdots + x_n}{n}
$$

\end{definition}

\begin{definition}[Median]
    For a dataset with $n$ values arranged in ascending order, the median is defined as:
    \[
    \text{median} =
    \begin{cases}
        x_{\frac{n+1}{2}} & \text{if } n \text{ is odd} \\
        \frac{x_{\frac{n}{2}} + x_{\frac{n}{2}+1}}{2} & \text{if } n \text{ is even}
    \end{cases}
    \]
\end{definition}

\begin{definition}[Mode]
The \textbf{mode} is the value that appears most frequently in a dataset. A dataset can have:
\begin{itemize}
    \item \textbf{No mode}: If all values appear with equal frequency
    \item \textbf{One mode (unimodal)}: If one value appears most frequently
    \item \textbf{Multiple modes (multimodal)}: If two or more values tie for the highest frequency
\end{itemize}
\end{definition}

\begin{definition}[Range]
The \textbf{range} of a dataset is the difference between the maximum and minimum values:
\[
\text{Range} = x_{\max} - x_{\min}
\]
\end{definition}

\begin{definition}[Sample Variance and Standard Deviation]
For a sample with $n$ values, the \textbf{sample variance}, denoted by $s^2$, is defined as:
$$
s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2
$$
The \textbf{sample standard deviation}, denoted by $s$, is the square root of the variance:
$$
s = \sqrt{s^2} = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$
\end{definition}

\begin{definition}[Coefficient of Variation]
The \textbf{coefficient of variation} is the ratio of standard deviation to the mean:
\[
CV = \frac{s}{\bar{x}} \times 100\%
\]
\end{definition}

\begin{definition}[Percentile]
The $p$-th \textbf{percentile} is the value below which $p\%$ of the data falls.
\end{definition}

\begin{definition}[Quartiles]

Quartiles divide an ordered dataset into four equal parts.

\begin{itemize}
    \item \textbf{Q1 (First Quartile)}: The 25th percentile. 25\% of the data is less than this value.
    \item \textbf{Q2 (Second Quartile)}: The 50th percentile. This is the \textbf{median} of the dataset.
    \item \textbf{Q3 (Third Quartile)}: The 75th percentile. 75\% of the data is less than this value.
    \item \textbf{Q4 (Fourth Quartile)}: The 100th percentile. 100\% of the data is less than this value.
\end{itemize}
\end{definition}

\begin{definition}[Interquartile Range]
    The \textbf{interquartile range (IQR)} measures the spread of the middle 50\% of the data and is defined as
    $$
    IQR = Q_3 - Q_1,
    $$
    where $Q_1$ and $Q_3$ are the first and third quartiles. A larger IQR indicates that the central portion of the data is more widely dispersed, while a smaller IQR indicates that it is more tightly clustered.
\end{definition}

\begin{definition}[Normal Distribution]
A \textbf{normal distribution} is a symmetric, bell-shaped distribution where the mean, median, and mode are all equal and located at the center. Its shape is determined entirely by its mean ($\mu$) and standard deviation ($\sigma$).
\end{definition}

\begin{definition}[Skewness]
    \textbf{Skewness} measures the asymmetry of data distribution:
    \begin{itemize}
        \item \textbf{Positive skew (skewed to the right)}: Tail extends to the right (mean > median)
        \item \textbf{Negative skew (skewed to the left)}: Tail extends to the left (mean < median)
        \item \textbf{Symmetric}: Mean $\approx$ median
    \end{itemize}
\end{definition}

\begin{definition}[Box Plot Components]
A box plot displays five key statistics:
\begin{itemize}
    \item \textbf{Minimum}: The smallest value (excluding outliers)
    \item \textbf{Q1 (First Quartile)}: 25\% of data below this value
    \item \textbf{Median (Q2)}: 50\% of data below this value
    \item \textbf{Q3 (Third Quartile)}: 75\% of data below this value
    \item \textbf{Maximum}: The largest value (excluding outliers)
    \item \textbf{Outliers}: Points beyond 1.5 × IQR from the box
\end{itemize}
\end{definition}

\begin{theorem}[Law of Large Numbers]
    The sample mean of a dataset will converge to the population mean as the sample size increases.
\end{theorem}

\begin{theorem}[The Empirical Rule]
If a dataset is approximately normal with a sample mean $\bar{x}$ and sample standard deviation $s$, then:
\begin{itemize}
    \item Approximately \textbf{68.3\%} of the observations lie within 1 standard deviation of the mean ($\bar{x} \pm s$).
    \item Approximately \textbf{95.4\%} of the observations lie within 2 standard deviations of the mean ($\bar{x} \pm 2s$).
    \item Approximately \textbf{99.7\%} of the observations lie within 3 standard deviations of the mean ($\bar{x} \pm 3s$).
\end{itemize}
\end{theorem}

% ---------- 07-09 Linear Algebra ----------

% 7

\begin{definition}{Linear Equations} A linear equation in the variables $x_1, x_2, \ldots, x_n$ is an equation that can be written in the form
\[
a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = b
\]

where $b$ and the coefficients $a_1, \ldots, a_n$ are constants.

\end{definition}

\begin{definition}{Systems of Linear Equations} A \textbf{system of linear equations} (also called a \textbf{linear system}) is a collection of one or more linear equations involving the same set of variables. \end{definition}

\begin{definition}{Solutions to a System of Linear Equations} A \textbf{solution} of a linear system in the variables $x_1, x_2, \ldots, x_n$ is a list of numbers $(s_1, s_2, \ldots, s_n)$ that satisfies all equations of the system when substituted for the variables $x_1, x_2, \ldots, x_n$, respectively. The set of all possible solutions is called its \textbf{solution set}. Two linear systems are called \textbf{equivalent} if they have the same solution set. \end{definition}

\begin{definition} A \textbf{matrix} is a rectangular array of numbers arranged in rows and columns. The \textbf{coefficient matrix} of a linear system contains only the coefficients of the variables, while the \textbf{augmented matrix} includes an additional column for the constants from the right-hand side of the equations. \end{definition}

\begin{definition} The \textbf{size} of a matrix is defined by the number of its rows and columns, expressed as \emph{rows} $\times$ \emph{columns}. For instance, the coefficient matrix above is of size $3 \times 3$, and the augmented matrix is of size $3 \times 4$. \end{definition}

\begin{definition}{Echelon Forms}
A matrix is in \textbf{echelon form} if it satisfies the following conditions:
\begin{enumerate}
    \item All zero rows are at the bottom.
    \item Each leading entry of a row is in a column to the right of the leading entry of the row above it.
    \item All entries in a column below a leading entry are zeros.
\end{enumerate}

In addition to echelon form, a matrix may also be in \textbf{reduced row echelon form} (RREF), which has the following properties:
\begin{enumerate}[resume]
    \item The leading entry in each nonzero row is 1.
    \item Each leading 1 is the only nonzero entry in its column.
\end{enumerate}
\end{definition}

\begin{theorem}{Existence Theorem}
A linear system is consistent if and only if an echelon form of the augmented matrix has no row of the form
    \[
    \left[\begin{array}{llll}
    0 & \ldots & 0 & b
    \end{array}\right]
    \]
where $b$ is nonzero.
\end{theorem}

\begin{theorem}{Uniqueness of Reduced Echelon Form}
    Each matrix is row equivalent to one and only one reduced echelon matrix
\end{theorem}

\begin{theorem}{Uniqueness Theorem}
    If a linear system is consistent, then the solution set contains either
    \begin{enumerate}[label=(\roman*)]
        \item a unique solution, when there are no free variables, or
        \item infinitely many solutions, when there is at least one free variable.
    \end{enumerate}
\end{theorem}

% 8

\begin{definition}
    For each positive integer $n$, we let $\mathbb{R}^n$ denote the collection of ordered $n$-tuples with each entry in $\mathbb{R}$. We often write these elements as $n \times 1$ matrices. We define addition and scalar multiplication of vectors in $\mathbb{R}^n$ in the same way as we do for $\mathbb{R}^2$. That is, we go coordinate-by-coordinate.
\end{definition}

\begin{definition}{Linear Combinations}
    Given a set of vectors $\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_p \in \mathbb{R}^n$ and scalars $c_1, c_2, \ldots, c_p \in \mathbb{R}$, the vector $\vect{y}$ given by

\[
\vect{y} = c_1 \vect{v}_1 + \cdots + c_p \vect{v}_p
\]

is called a linear combination of $\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_p$ with weights $c_1, c_2, \ldots, c_p$.

\end{definition}

\begin{definition}{Span of a Set of Vectors}
    If $\vect{v}_1, \ldots, \vect{v}_p$ are in $\mathbb{R}^n$, then the set of all linear combinations of $\vect{v}_1, \ldots, \vect{v}_p$ is denoted by \text{Span} $\left\{\vect{v}_1, \ldots, \vect{v}_p\right\}$ and is called the subset of $\mathbb{R}^n$ spanned by $\vect{v}_1, \ldots, \vect{v}_p$. In other words, the span of $\vect{v}_1, \ldots, \vect{v}_p$ is all vectors that can be written in the form
    $$
    c_1 \vect{v}_1 + \cdots + c_p \vect{v}_p
    $$
    with $c_1, \ldots, c_p$ scalars.

\end{definition}

\begin{definition}{Matrix Equation}
  If $A$ is an $m \times n$ matrix with columns $\vect{a}_1, \ldots, \vect{a}_n$, and if $\vect{x} \in \mathbb{R}^n$, then the product of $A$ and $\vect{x}$, denoted by $A \vect{x}$, is

\[
A \vect{x} = \left[\begin{array}{llll}
\vect{a}_1 & \vect{a}_2 & \ldots & \vect{a}_n
\end{array}\right] \left[\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}\right] = x_1 \vect{a}_1 + x_2 \vect{a}_2 + \cdots + x_n \vect{a}_n
\]
    
\end{definition}

\begin{definition}
    An indexed set of vectors $\left\{\vect{v}_1, \ldots, \vect{v}_p\right\} \subset \mathbb{R}^n$ is said to be \textbf{linearly independent} if the vector equation
    
    \[
    x_1 \vect{v}_1 + \cdots + x_p \vect{v}_p = \vect{0}
    \]
    
    has only the trivial solution, i.e., if the only solution is $\left(x_1, \ldots, x_p\right) = (0, \ldots, 0)$. Likewise, the set $\left\{\vect{v}_1, \ldots, \vect{v}_p\right\}$ is said to be \textbf{linearly dependent} if there exist weights $c_1, \ldots, c_p$, not all zero, such that
    
    \[
    c_1 \vect{v}_1 + \cdots + c_p \vect{v}_p = \vect{0}
    \]
    
    We call such an equation a \textit{linear dependence relation} when the weights are not all zero.
\end{definition}

\begin{theorem}
    If $A$ is an $m \times n$ matrix, with columns $\vect{a}_1, \ldots, \vect{a}_n \in \mathbb{R}^m$ and if $\vect{b} \in \mathbb{R}^m$, the matrix equation

\[
A \vect{x} = \vect{b}
\]

has the same solution set as the vector equation

\[
x_1 \vect{a}_1 + x_2 \vect{a}_2 + \cdots + x_n \vect{a}_n = \vect{b}
\]

which, in turn, has the same solution set as the system of linear equations with augmented matrix

\[
\left[\begin{array}{lllll}
\vect{a}_1 & \vect{a}_2 & \ldots & \vect{a}_n & \vect{b}
\end{array}\right].
\]

\end{theorem}

\begin{theorem}
Let \( A \) be an \( m \times n \) matrix. Then the following statements are equivalent:
\begin{enumerate}[label=(\alph*)]
    \item For each \( \vect{b} \in \mathbb{R}^m \), the equation \( A \vect{x} = \vect{b} \) has a solution.
    \item Each \( \vect{b} \in \mathbb{R}^m \) is a linear combination of the columns of \( A \).
    \item The columns of \( A \) span \( \mathbb{R}^m \).
    \item The matrix \( A \) has a pivot position in every row.
\end{enumerate}
\end{theorem}

\begin{theorem}
If \( A \) is an \( m \times n \) matrix, \( \vect{u} \) and \( \vect{v} \) are vectors in \( \mathbb{R}^n \), and \( c \) is a scalar, then:

\begin{enumerate}[label=(\alph*)]
    \item \( A(\vect{u} + \vect{v}) = A \vect{u} + A \vect{v} \)  


    \item \( A(c \vect{u}) = c(A \vect{u}) \)  

\end{enumerate}
\end{theorem}

\begin{theorem}
    Suppose $A \vect{x} = \vect{b}$ is consistent for some $\vect{b}$, and let $\vect{p}$ be a solution. Then the solution set of $A \vect{x} = \vect{b}$ is the set of all vectors of the form
    \[
    \vect{w} = \vect{p} + \vect{v}_h,
    \]
    where $\vect{v}_h$ is any solution of the homogeneous equation $A \vect{x} = \vect{0}$.
\end{theorem}

\begin{theorem}
    \begin{enumerate}[(a)]
        \item If a set of vectors contains the zero vector, then the set is linearly dependent.
        \item If a set of vectors contains a scalar multiple of another vector, then the set is linearly dependent.
        \item If a set of vectors contains more vectors than there are entries in each vector, then the set is linearly dependent.
    \end{enumerate}
\end{theorem}

% 9

\begin{definition}{Matrix Addition}
    Let $A$ and $B$ be $m \times n$ matrices. The \textbf{sum} of $A$ and $B$, denoted $A + B$, is the $m \times n$ matrix whose entries are obtained by adding the corresponding entries of $A$ and $B$.

    Given matrices

    $$
    A=\left[\begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1 n} \\
    a_{21} & a_{22} & \cdots & a_{2 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \cdots & a_{m n}
    \end{array}\right], \quad B=\left[\begin{array}{cccc}
    b_{11} & b_{12} & \cdots & b_{1 n} \\
    b_{21} & b_{22} & \cdots & b_{2 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    b_{m 1} & b_{m 2} & \cdots & b_{m n}
    \end{array}\right]
    $$

    both of the same dimension $m \times n$, the sum $A+B$ is thus defined as

    $$
    A+B=\left[\begin{array}{cccc}
    a_{11}+b_{11} & a_{12}+b_{12} & \cdots & a_{1 n}+b_{1 n} \\
    a_{21}+b_{21} & a_{22}+b_{22} & \cdots & a_{2 n}+b_{2 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m 1}+b_{m 1} & a_{m 2}+b_{m 2} & \cdots & a_{m n}+b_{m n}
    \end{array}\right]
    $$
\end{definition}

\begin{definition}{Scalar-Matrix Multiplication}
    Let $A$ be an $m \times n$ matrix and $c$ be a scalar. The \textbf{product} of $c$ and $A$, denoted $cA$, is the $m \times n$ matrix whose entries are obtained by multiplying each entry of $A$ by $c$, and is thus defined as

    $$
    c A=\left[\begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1 n} \\
    a_{21} & a_{22} & \cdots & a_{2 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \cdots & a_{m n}
    \end{array}\right] = \left[\begin{array}{cccc}
    c a_{11} & c a_{12} & \cdots & c a_{1 n} \\
    c a_{21} & c a_{22} & \cdots & c a_{2 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    c a_{m 1} & c a_{m 2} & \cdots & c a_{m n}
    \end{array}\right]
    $$
    
\end{definition}

\begin{definition}
    For $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$, with $B=\left[\begin{array}{lll}\vect{b_1} & \vect{b_2} \cdots & \vect{b_p}\end{array}\right]$, we define the product $A B$ by the formula

\[
A B=\left[\begin{array}{llll}
A \vect{b_1} & A \vect{b_2} & \cdots & A \vect{b_p}
\end{array}\right]
\]

\end{definition}

\begin{definition}
    Let $A$ be a square matrix, i.e. $ A \in \mathbb{R}^{n \times n}$. The $k$th power of $A$, denoted $A^k$, is defined as the product of $A$ with itself $k$ times. That is,

    $$
    A^k=\underbrace{ AAA \cdot \cdots A}_{k \text { times }}
    $$

    where $A$ appears $k$ times on the right-hand side.
\end{definition}

\begin{definition}
    Given a matrix $A \in \mathbb{R}^{m \times n}$, the transpose of $A$ is the matrix $A^T$ whose $i$th column is the $i$th row of $A$.
\end{definition}

\begin{definition}
    A square matrix \( A \in \mathbb{R}^{n \times n} \) is invertible (or \textbf{nonsingular}) if there exists a matrix \( C \in \mathbb{R}^{n \times n} \) such that \( A C = C A = I_n \). The matrix \( C \) is called the inverse of \( A \) and is denoted by \( A^{-1} \). Thus, \( A^{-1} A = A A^{-1} = I_n \).
\end{definition}

\begin{theorem}

    Let $A$, $B$, $C$ be matrices of the same size and let $\alpha$, $\beta$ be scalars. Then
    \begin{enumerate}[(a)]
        \item $A+B=B+A$
        \item $(A+B)+C=A+(B+C)$
        \item $A+0=A$
        \item $\alpha(A+B)=\alpha A+\alpha B$
        \item $(\alpha+\beta) A=\alpha A+\beta A$
        \item $\alpha(\beta A)=(\alpha \beta) A$
    \end{enumerate}
\end{theorem}

\begin{theorem}
        Let $A, B, C$ be matrices, of appropriate dimensions, and let $\alpha$ be a scalar. Then
\begin{enumerate}[(a)]
    \item $A(BC) = (AB)C$
    \item $A(B + C) = AB + AC$
    \item $(B + C)A = BA + CA$
    \item $\alpha(AB) = (\alpha A)B = A(\alpha B)$
    \item $I_n A = AI_n = A$
\end{enumerate}
\end{theorem}

\begin{theorem}
    Let $A$ and $B$ be matrices of appropriate dimensions and let $\alpha$ be a scalar. Then
    \begin{enumerate}[(a)]
        \item $(A^T)^T = A$
        \item $(A + B)^T = A^T + B^T$
        \item $(\alpha A)^T = \alpha A^T$
        \item $(AB)^T = B^T A^T$
    \end{enumerate}
\end{theorem}

\begin{theorem} Let $A$ and $B$ be invertible $n \times n$ matrices. Then:
\begin{enumerate}[(a)]
    \item $A^{-1}$ is invertible, with
    \[
    \left(A^{-1}\right)^{-1}=A
    \]
    \item The product $A B$ is invertible, with
    \[
    (A B)^{-1}=B^{-1} A^{-1}
    \]
    \item The transpose of $A$ is also invertible, i.e. $A^T$ is invertible, with
    \[
    \left(A^T\right)^{-1}=\left(A^{-1}\right)^T
    \]
\end{enumerate}

\end{theorem}

\begin{theorem}
    Let $A=\left[\begin{array}{ll}
    a & b \\
    c & d
    \end{array}\right]$. If $ad-bc \neq 0$, then the inverse of $A$ is given by

    \[
    A^{-1}=\frac{1}{ad-bc}\left[\begin{array}{cc}
    d & -b \\
    -c & a
    \end{array}\right]
    \]
\end{theorem}

\begin{theorem}
    An $n \times n$ matrix $A$ is invertible if and only if $A$ is row equivalent to $I_n$. In this case, any sequence of elementary row operations that reduces $A$ to $I_n$ also transforms $I_n$ into $A^{-1}$.
\end{theorem}

\begin{theorem}
    Let \( A \) be an invertible matrix. Then the equation \( A \vect{x} = \vect{b} \) has a unique solution given by \( \vect{x} = A^{-1} \vect{b} \).
\end{theorem}

\begin{theorem}
    Let \( A \) be an \( n \times n \) matrix. The following statements are equivalent:
    \begin{enumerate}[(a)]
        \item $A$ is invertible.
        \item $A$ is row equivalent to $I_n$.
        \item $A$ has $n$ pivot positions (i.e. one for each row and column).
        \item The equation $A \vect{x}=\overline{0}$ has only the trivial solution.
        \item The columns of $A$ are linearly independent.
        \item The equation \( A \vect{x} = \vect{b} \) has a unique solution for each \( \vect{b} \in \mathbb{R}^n \).
        \item The columns of $A \operatorname{span} \mathbb{R}^n$.
        \item \( \text{det}(A) \neq 0 \).
        \item There is an $n \times n$ matrix $C$ such that $C A=I$.
        \item There is an $n \times n$ matrix $D$ such that $A D=I$.
        \item $A^T$ is invertible.
    \end{enumerate}
\end{theorem}

% ---------- 10-11 Calculus ----------

% 10

\begin{definition}[Limit]
Let \( f(x) \) be a function defined near a point \( c \). We say that the \textbf{limit} of \( f(x) \) as \( x \) approaches \( c \) is \( L \), written as
\vspace{0.5em}
\[
\lim_{x \to c} f(x) = L
\]
\vspace{0.5em}
if we can make the value of \( f(x) \) arbitrarily close to \( L \) by choosing an \( x \) that is sufficiently close to \( c \), but not equal to \( c \).
\end{definition}

\begin{definition}[Continuity]
A function \( f \) is \textbf{continuous} at a point \( c \) if the following three conditions are met:
\begin{enumerate}
    \item \( f(c) \) is defined.
    \item \( \lim_{x \to c} f(x) \) exists.
    \item \( \lim_{x \to c} f(x) = f(c) \).
\end{enumerate}
A function is continuous on an interval if it is continuous at every point in that interval.
\end{definition}

\begin{definition}[The Derivative]
The \textbf{derivative} of a function \( f \) with respect to \( x \), denoted \( f'(x) \), is the function
\[
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]
provided the limit exists. If \( f'(x) \) exists, we say that \( f \) is \textbf{differentiable} at \( x \).
\end{definition}

\begin{definition}
     For a function $y=f(x)$ the points on the graph where the graph has zero slope are called stationary points. In other words stationary points are where $f^{\prime}(x)=0$.
    \end{definition}

\begin{definition}[The Second Derivative]
The \textbf{second derivative} of a function \( f \), denoted \( f''(x) \), is the function
\[
f''(x) = \frac{d}{dx} \left( f'(x) \right)
\]
provided the limit exists.
\end{definition}

\begin{theorem}{Constant and Power Rules}
    \begin{enumerate}
        \item \textbf{Constant Rule:} If $c$ is constant, then $\dfrac{d}{dx}(c) = 0$, \vspace{0.5em}
        \item \textbf{Power Rule:} For any real $n$, $\dfrac{d}{dx}x^n = n x^{n - 1}$. \vspace{0.5em}
    \end{enumerate}
\end{theorem}

\begin{theorem}{Scalar and Linearity Rules}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \vspace{0.5em}
        \item \textbf{Scalar Rule:} \( \frac{d}{dx}[c \cdot f(x)] = c \cdot f'(x) \). \vspace{0.5em}
        \item \textbf{Linearity Rule:} \( \frac{d}{dx}[f(x) \pm g(x)] = f'(x) \pm g'(x) \). \vspace{0.5em}
    \end{enumerate}
\end{theorem}

\begin{theorem}{Product and Quotient Rules}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \vspace{0.5em}
        \item \textbf{Product Rule:} If \(f(x)\) and \(g(x)\) are differentiable, then \vspace{0.5em}
        \[
        \frac{d}{dx}\big(f(x)g(x)\big)=f'(x)g(x)+f(x)g'(x). \vspace{0.5em}
        \] \vspace{0.5em}
        \item \textbf{Quotient Rule:} If \(f(x)\) and \(g(x)\) are differentiable and \(g(x)\neq0\), then
        \vspace{0.5em}
        \[
        \frac{d}{dx}\left(\frac{f(x)}{g(x)}\right)=\frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}. \vspace{0.5em}
        \]
    \end{enumerate}
\end{theorem}

\begin{theorem}{Chain Rule}
    \vspace{0.5em}
    \setcounter{enumi}{6}
    \begin{enumerate}
        \item If \( h(x) = f(g(x)) \) is a composite function, then its derivative is the derivative of the outer function (evaluated at the inner function) multiplied by the derivative of the inner function. \vspace{0.5em}
        \[
        h'(x) = f'(g(x)) \cdot g'(x). \vspace{0.5em}
        \]
    \end{enumerate}
\end{theorem}

\begin{theorem}{Rules of Differentiation}

    \begin{enumerate}
        \item \textbf{Constant Rule:} If $c$ is constant, then $\dfrac{d}{dx}c=0$. \vspace{0.5em}
        \item \textbf{Power Rule:} For any real $n$, $\dfrac{d}{dx}x^n = n x^{n-1}$. \vspace{0.5em}
        \[
        \frac{d}{dx}x^n = n x^{n-1}.
        \]
        \item \textbf{Scalar Rule:} If $c$ is constant, then $\dfrac{d}{dx}[c \cdot f(x)] = c \cdot f'(x)$. \vspace{0.5em}
        \item \textbf{Linearity:} For differentiable $f,g$ and constants $a,b$, $\dfrac{d}{dx}(a f + b g) = a f' + b g'$. \vspace{0.5em}
        \item \textbf{Product Rule:} If $f,g$ are differentiable, then $\dfrac{d}{dx}(f g) = f'g + fg'$. \vspace{0.5em}
        \item \textbf{Quotient Rule:} If $f,g$ are differentiable and $g(x)\neq0$, then $\dfrac{d}{dx}\left(\frac{f}{g}\right) = \frac{f'g - fg'}{g^2}$. \vspace{0.5em}
        \item \textbf{Chain Rule:} If $f,g$ are differentiable, then $\dfrac{d}{dx}(f(g(x))) = f'(g(x))g'(x)$. \vspace{0.5em}
    \end{enumerate}

\end{theorem}

\begin{theorem}[The First Derivative Test]
    Let \(c\) be a critical point of a continuous function \(f\).
    \begin{itemize}
        \item If \(f'(x)\) changes from negative to positive at \(c\), then \(f\) has a \textbf{local minimum} at \(c\).
        \item If \(f'(x)\) changes from positive to negative at \(c\), then \(f\) has a \textbf{local maximum} at \(c\).
        \item If \(f'(x)\) does not change sign at \(c\), then \(f\) has no local extremum at \(c\); it is a stationary point of inflection.
    \end{itemize}

\end{theorem}

\begin{theorem}[The Second Derivative Test]
    Let \(c\) be a stationary point of \(f\) (i.e., \(f'(c) = 0\)).
    \begin{itemize}
        \item If \(f''(c) > 0\), then \(f\) has a \textbf{local minimum} at \(c\).
        \item If \(f''(c) < 0\), then \(f\) has a \textbf{local maximum} at \(c\).
        \item If \(f''(c) = 0\), the test is inconclusive.
    \end{itemize}
\end{theorem}

% 11

\begin{definition}[Function of Two Variables]
A \emph{function of two variables} is a rule that assigns to each ordered pair \((x, y)\) in a set \(D \subseteq \mathbb{R}^2\) a unique real number \(z\), which we write as \(z = f(x, y)\). The set \(D\) is called the \emph{domain} of \(f\).

The \emph{range} of \(f\) is the set of all real numbers \(z\) for which there exists at least one \((x, y) \in D\) such that \(f(x, y) = z\) which is illustrated in \autoref{fig:ch11-domain-range}.
\end{definition}

\begin{definition}[Level Curve and Contour Map]
A \textbf{level curve} of a function \(f(x, y)\) is the set of all points \((x, y)\) in the input plane where the function has a constant value, i.e., \(f(x, y) = c\) for some constant \(c\).

A collection of level curves for different values of \(c\) forms a \textbf{contour map}.
\end{definition}

\begin{definition}[Partial Derivatives]
The \textbf{partial derivative} of \( f(x, y) \) with respect to \( x \), denoted \( \frac{\partial f}{\partial x} \) or \( f_x \), is
$$
\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h, y) - f(x, y)}{h}
$$
The \textbf{partial derivative} with respect to \( y \), denoted \( \frac{\partial f}{\partial y} \) or \( f_y \), is
$$
\frac{\partial f}{\partial y} = \lim_{h \to 0} \frac{f(x, y+h) - f(x, y)}{h}
$$
\end{definition}

\begin{definition}[The Gradient]
The \textbf{gradient} of a function \( f(x, y) \), denoted \( \nabla f \), is the vector function defined by:

$$
\nabla f(x, y) = \left\langle \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right\rangle = \frac{\partial f}{\partial x} \mathbf{i} + \frac{\partial f}{\partial y} \mathbf{j}
$$

\end{definition}

% ---------- 12+ Number Theory and other universal concepts ----------

